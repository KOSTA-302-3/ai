import modal
import sys
import os

batch_image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "torch", 
        "torchvision", 
        "transformers", 
        "pillow", 
        "pymysql", 
        "sqlalchemy", 
        "redis", 
        "qdrant-client", 
        "scikit-learn",
        "requests",
        "numpy",
        "accelerate",
        "sentencepiece"
    )
)

app = modal.App("santa-batch", image=batch_image)

model_volume = modal.Volume.from_name("santa-models", create_if_missing=True)
secrets = [modal.Secret.from_name("santa-aws-secret")]

MODEL_PATH = "/models/siglip_best.pth"

@app.function(
    gpu="T4",
    volumes={"/models": model_volume},
    secrets=secrets,
    timeout=3600
)
def run_batch_recalculation():
    """
    Centroid ì¬ê³„ì‚°
    - posts í…Œì´ë¸”ê³¼ image_sources í…Œì´ë¸”ì„ JOINí•˜ì—¬ ë°ì´í„° ì¡°íšŒ
    - post_level (FLOAT) -> intë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©
    - SigLIPìœ¼ë¡œ ë©€í‹° ëª¨ë‹¬ ë²¡í„° ìƒì„± -> í†µí•© ë²¡í„° -> Centroid ê°±ì‹ 
    """
    import torch
    import numpy as np
    import requests
    import json
    import redis
    import pymysql
    from PIL import Image
    from io import BytesIO
    from sqlalchemy import create_engine, text
    from transformers import AutoModel, AutoProcessor
    from qdrant_client import QdrantClient, models

    print("[Batch] ë©€í‹°ëª¨ë‹¬ Centroid ì¬ê³„ì‚° ì‘ì—… ì‹œì‘ (Schema Sync)")

    # ---------------------------------------------------------
    # 1. DB ë° Redis ì—°ê²°
    # ---------------------------------------------------------
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    db_url = f"mysql+pymysql://{os.environ['MYSQL_USER']}:{os.environ['MYSQL_PASSWORD']}@{os.environ['MYSQL_HOST']}:{os.environ['MYSQL_PORT']}/{os.environ['MYSQL_DB']}"
    engine = create_engine(db_url)

    r = redis.Redis(
        host=os.environ['REDIS_HOST'], 
        port=int(os.environ['REDIS_PORT']), 
        decode_responses=True
    )

    # ---------------------------------------------------------
    # 2. ëª¨ë¸ ë¡œë“œ
    # ---------------------------------------------------------
    print("ğŸ§  SigLIP ëª¨ë¸ ë¡œë”© ì¤‘...")
    model_name = "google/siglip-so400m-patch14-384"
    processor = AutoProcessor.from_pretrained(model_name)
    model = AutoModel.from_pretrained(model_name).to(device)
    
    if os.path.exists(MODEL_PATH):
        print(f"ğŸ“‚ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ: {MODEL_PATH}")
        model.load_state_dict(torch.load(MODEL_PATH, map_location=device), strict=False)
    
    model.eval()

    # ---------------------------------------------------------
    # 3. ë°ì´í„° ë¡œë“œ (Posts + Image Sources JOIN)
    # ---------------------------------------------------------
    print("ğŸ“¥ RDS ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    query_str = """
        SELECT 
            p.post_id,
            p.content,
            CAST(p.post_level AS UNSIGNED) as level,
            JSON_ARRAYAGG(i.source) as image_urls
        FROM posts p
        LEFT JOIN image_sources i ON p.post_id = i.post_id
        WHERE p.post_level BETWEEN 1 AND 10
        GROUP BY p.post_id, p.content, p.post_level
    """

    with engine.connect() as conn:
        posts = conn.execute(text(query_str)).fetchall()

    print(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ ê²Œì‹œë¬¼: {len(posts)}ê°œ")
    
    level_vectors_map = {i: [] for i in range(1, 11)}
    
    success_cnt = 0
    fail_cnt = 0

    # ---------------------------------------------------------
    # 4. ë£¨í”„: ê²Œì‹œë¬¼ë³„ í†µí•© ë²¡í„° ìƒì„±
    # ---------------------------------------------------------
    for row in posts:
        pid, content, level, img_urls_json = row
        
        # levelì´ float->int ë³€í™˜ ê³¼ì •ì—ì„œ ë²”ìœ„ ë²—ì–´ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „ì¥ì¹˜
        if not (1 <= level <= 10):
            continue

        temp_vectors = []

        try:
            # A. ì´ë¯¸ì§€ ë²¡í„°í™” (JSON ë¬¸ìì—´ íŒŒì‹±)
            if img_urls_json:
                try:
                    # MySQL JSON_ARRAYAGG ê²°ê³¼ê°€ ë¬¸ìì—´ë¡œ ë„˜ì–´ì˜¤ë©´ íŒŒì‹±
                    url_list = json.loads(img_urls_json) if isinstance(img_urls_json, str) else img_urls_json
                    
                    # null ê°’ì´ ë¦¬ìŠ¤íŠ¸ì— ì„ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§
                    url_list = [u for u in url_list if u]

                    for url in url_list:
                        try:
                            res = requests.get(url, timeout=5)
                            if res.status_code == 200:
                                img = Image.open(BytesIO(res.content)).convert("RGB")
                                inputs = processor(images=img, return_tensors="pt").to(device)
                                with torch.no_grad():
                                    v = model.get_image_features(**inputs).cpu().numpy()[0]
                                    temp_vectors.append(v)
                        except Exception:
                            continue 
                except Exception as e:
                    print(f"âš ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (ID: {pid}): {e}")

            # B. í…ìŠ¤íŠ¸ ë²¡í„°í™”
            if content and isinstance(content, str) and len(content.strip()) > 0:
                text_inputs = processor(text=[content], padding="max_length", truncation=True, return_tensors="pt").to(device)
                with torch.no_grad():
                    v_text = model.get_text_features(**text_inputs).cpu().numpy()[0]
                    temp_vectors.append(v_text)

            # C. í†µí•© (Mean & Normalize)
            if temp_vectors:
                combined_vec = np.mean(temp_vectors, axis=0)
                
                norm = np.linalg.norm(combined_vec)
                if norm > 0:
                    final_vector = combined_vec / norm
                else:
                    final_vector = combined_vec

                level_vectors_map[level].append(final_vector)
                success_cnt += 1
            else:
                fail_cnt += 1

        except Exception as e:
            print(f"ì¹˜ëª…ì  ì—ëŸ¬ (ID: {pid}): {e}")
            fail_cnt += 1
            
        if (success_cnt + fail_cnt) % 50 == 0:
            print(f"ì§„í–‰ë¥ : {success_cnt + fail_cnt}/{len(posts)}")

    # ---------------------------------------------------------
    # 5. Centroid ê³„ì‚° ë° ì €ì¥
    # ---------------------------------------------------------
    print("Centroid ì‚°ì¶œ ì¤‘...")
    new_centroids = {}

    for lvl in range(1, 11):
        vecs = np.array(level_vectors_map[lvl])
        
        if len(vecs) > 0:
            mean_v = np.mean(vecs, axis=0)
            norm_v = mean_v / np.linalg.norm(mean_v)
            new_centroids[str(lvl)] = norm_v.tolist()
            print(f"  - Level {lvl}: {len(vecs)}ê°œ ê²Œì‹œë¬¼ ì‚¬ìš©")
        else:
            print(f"Level {lvl}: ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ê°±ì‹  ìŠ¤í‚µ")

    if new_centroids:
        r.set("system:centroids", json.dumps(new_centroids))
        print(f"Centroid ì—…ë°ì´íŠ¸ ì™„ë£Œ! (ì´ {len(new_centroids)}ê°œ ë ˆë²¨)")
    else:
        print("ê°±ì‹ ëœ Centroidê°€ ì—†ìŠµë‹ˆë‹¤.")

    return {"status": "success", "updated_levels": list(new_centroids.keys())}

if __name__ == "__main__":
    with app.run():
        run_batch_recalculation.remote()